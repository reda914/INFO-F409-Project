{
 "cells": [
  {
   "cell_type": "code",
   "id": "6f3cea02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.330905Z",
     "start_time": "2025-12-27T13:49:13.325057Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "num_agents = 10                   \n",
    "num_rounds = 200\n",
    "num_episodes = 500\n",
    "b = [2, 5, 10]           # Benefit\n",
    "c = 1                    # Cost of cooperation\n",
    "\n",
    "# Learning parameters        \n",
    "chi = 10 / num_episodes    # Reputation assignment error\n",
    "#chi = 0.01"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "03e44c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.370167Z",
     "start_time": "2025-12-27T13:49:13.356360Z"
    }
   },
   "source": [
    "class MatrixGame(ParallelEnv):\n",
    "    def __init__(self, reward_matrix, agents, norm):\n",
    "        self.agents = agents \n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.norm = norm\n",
    "        self.last_opponent = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.states = {agent: np.random.choice([0,1]) for agent in self.agents}\n",
    "\n",
    "        return self.states\n",
    "    \n",
    "    def get_action_rules(self, action_rule):\n",
    "        # Convert action_rule_id to 4-bits\n",
    "        bits = [(action_rule >> i) & 1 for i in range(4)]  # bits[0]=LSB, bits[3]=MSB\n",
    "        return bits\n",
    "    \n",
    "\n",
    "    def determine_state(self, focal_action, opponent_state):\n",
    "        if focal_action == 0 and opponent_state == 0:\n",
    "            return self.norm[3]  # Bit 3\n",
    "        elif focal_action == 0 and opponent_state == 1:\n",
    "            return self.norm[2]  # Bit 2\n",
    "        elif focal_action == 1 and opponent_state == 0:\n",
    "            return self.norm[1]  # Bit 1\n",
    "        else:  # (1,1)\n",
    "            return self.norm[0]  # Bit 0\n",
    "\n",
    "    def step(self):\n",
    "        pairings = []\n",
    "        players = self.agents.copy()\n",
    "        for _ in range(num_agents//2):\n",
    "            index = random.randrange(len(players))\n",
    "            elem1 = players.pop(index)\n",
    "\n",
    "            index = random.randrange(len(players))\n",
    "            elem2 = players.pop(index)\n",
    "\n",
    "            pairings.append((elem1, elem2))\n",
    "\n",
    "\n",
    "        for pair in pairings:\n",
    "            action1 = pair[0].select_action(self.states[pair[1]])\n",
    "            action2 = pair[1].select_action(self.states[pair[0]])\n",
    "\n",
    "            self.actions[pair[0]] = action1\n",
    "            self.actions[pair[1]] = action2\n",
    "\n",
    "            self.last_opponent[pair[0]] = pair[1]\n",
    "            self.last_opponent[pair[1]] = pair[0]\n",
    "\n",
    "            reward1 = self.reward_matrix[action1][action2]\n",
    "            reward2 = self.reward_matrix[action2][action1]\n",
    "\n",
    "            self.rewards[pair[0]] = reward1\n",
    "            self.rewards[pair[1]] = reward2\n",
    "\n",
    "            state1 = self.determine_state(action1, self.states[pair[1]])\n",
    "            state2 = self.determine_state(action2, self.states[pair[0]])\n",
    "\n",
    "            if (random.random() < chi):\n",
    "                state1 = 1-state1\n",
    "            if (random.random() < chi):\n",
    "                state2 = 1-state2\n",
    "\n",
    "            self.states[pair[0]] = state1\n",
    "            self.states[pair[1]] = state2\n",
    "\n",
    "        return self.states, self.rewards, self.actions"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "fee9077c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.392723Z",
     "start_time": "2025-12-27T13:49:13.379535Z"
    }
   },
   "source": [
    "class Qlearner:\n",
    "    \"\"\"A Q-learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seeded=False,\n",
    "        action_size=2,\n",
    "        state_size=2,\n",
    "        learning_rate=0.01,\n",
    "        gamma=0.6,\n",
    "        epsilon=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.seeded = seeded\n",
    "        # initialize the Q-table: (State x Agent Action)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon # exploration\n",
    "\n",
    "        # tracking rewards/progress:\n",
    "        self.rewards_this_episode = []  # during an episode, save every time step's reward\n",
    "        self.episode_total_rewards = []  # each episode, sum the rewards, possibly with a discount factor\n",
    "        self.average_episode_total_rewards = []  # the average (discounted) episode reward to indicate progress\n",
    "\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset_agent(self):\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def select_greedy(self, state):\n",
    "        # np.argmax(self.qtable[state]) will select first entry if two or more Q-values are equal, but we want true randomness:\n",
    "        return np.random.choice(np.flatnonzero(np.isclose(self.qtable[state], self.qtable[state].max())))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.seeded:\n",
    "            return 5 \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action = self.select_greedy(state)\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, new_state, reward, done):\n",
    "        lr = self.learning_rate\n",
    "        self.qtable[state, action] += lr * (reward + (not done) * self.gamma * np.max(self.qtable[new_state]) - self.qtable[state, action])\n",
    "\n",
    "        self.rewards_this_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # track total reward:\n",
    "            episode_reward = self._calculate_episode_reward(self.rewards_this_episode, discount=False)\n",
    "            self.episode_total_rewards.append(episode_reward)\n",
    "\n",
    "            k = len(self.average_episode_total_rewards) + 1  # amount of episodes that have passed\n",
    "            self._calculate_average_episode_reward(k, episode_reward)\n",
    "            \n",
    "            # reset the rewards for the next episode:\n",
    "            self.rewards_this_episode = []\n",
    "\n",
    "    def _calculate_episode_reward(self, rewards_this_episode, discount=False):\n",
    "        if discount:\n",
    "            return sum([self.gamma**i * reward for i, reward in enumerate(rewards_this_episode)])\n",
    "        return sum(rewards_this_episode)\n",
    "\n",
    "    def _calculate_average_episode_reward(self, k, episode_reward):\n",
    "        if k > 1:  # running average is more efficient:\n",
    "            average_episode_reward = (1 - 1 / k) * self.average_episode_total_rewards[-1] + episode_reward / k\n",
    "        else:\n",
    "            average_episode_reward = episode_reward\n",
    "        self.average_episode_total_rewards.append(average_episode_reward)\n",
    "\n",
    "    def print_rewards(self, episode, print_epsilon=True, print_q_table=True):\n",
    "        # print(\"Episode \", episode + 1)\n",
    "        print(\"Total (discounted) reward of this episode: \", self.episode_total_rewards[episode])\n",
    "        print(\"Average total reward over all episodes until now: \", self.average_episode_total_rewards[-1])\n",
    "\n",
    "        print(\"Epsilon:\", self.epsilon) if print_epsilon else None\n",
    "        print(\"Q-table: \", self.qtable) if print_q_table else None"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "ef5997f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.427481Z",
     "start_time": "2025-12-27T13:49:13.419026Z"
    }
   },
   "source": [
    "def run(payoffs, norm):\n",
    "    run = 46\n",
    "    np.random.seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "    agents = [Qlearner(seeded=True) for _ in range(0)] + [Qlearner(seeded=False) for _ in range(10)]\n",
    "    env = MatrixGame(payoffs, agents, norm)\n",
    "    print(\"Norm:\", env.norm)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%100==0:\n",
    "            print(f\"Number of episodes: {episode}\")\n",
    "        obs = env.reset()\n",
    "        prev_obs = None\n",
    "        prev_actions = {}\n",
    "        prev_rewards = {}\n",
    "        prev_opponents = {}\n",
    "        for round in range(num_rounds):\n",
    "\n",
    "\n",
    "            next_obs, rewards, actions = env.step()\n",
    "            last_opponent = env.last_opponent\n",
    "            if prev_obs is not None:\n",
    "                for agent in agents:\n",
    "                    s = prev_obs[prev_opponents[agent]]\n",
    "                    a = prev_actions[agent]\n",
    "                    r = prev_rewards[agent]\n",
    "                    s_next = obs[last_opponent[agent]]\n",
    "                    agent.update(s, a, s_next, r, done=False)\n",
    "            prev_obs = obs\n",
    "            prev_actions = actions\n",
    "            prev_rewards = rewards\n",
    "            prev_opponents = last_opponent\n",
    "            obs = next_obs\n",
    "\n",
    "        for agent in agents:\n",
    "            s = prev_obs[prev_opponents[agent]]\n",
    "            a = prev_actions[agent]\n",
    "            r = prev_rewards[agent]\n",
    "            s_next = obs[last_opponent[agent]]\n",
    "\n",
    "            agent.update(s, a, s_next, r, done=True)\n",
    "\n",
    "    average_agent_round_payoff = np.zeros(num_episodes//2)\n",
    "    for agent in agents:\n",
    "        average_round_payoff = np.array(agent.episode_total_rewards[num_episodes//2:])/num_rounds\n",
    "        average_agent_round_payoff += average_round_payoff\n",
    "\n",
    "    b = payoffs[0,1]\n",
    "    average_agent_round_payoff = (average_agent_round_payoff.sum()/(10*(num_episodes//2)))/(b-c)\n",
    "\n",
    "    return average_agent_round_payoff"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "5c47c01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:52:21.179913Z",
     "start_time": "2025-12-27T13:49:13.427481Z"
    }
   },
   "source": [
    "results = []\n",
    "colors = [\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\",]\n",
    "\n",
    "effective_norm =[1,0,0,1]\n",
    "ineffective_norm =[0,0,0,0]\n",
    "\n",
    "payoffs = np.array([[0, b[0]],[-c, b[0]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[1]],[-c, b[1]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[2]],[-c, b[2]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "results = np.array(results)\n",
    "results *= 100\n",
    "\n",
    "x = np.arange(len(results))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(results)), results.flatten(), color=colors)\n",
    "plt.ylim(0,100)\n",
    "pair_labels = [\"2\", \"5\", \"10\"]\n",
    "pair_positions = [(x[i] + x[i+1])/2 for i in range(0, len(x), 2)]\n",
    "plt.xticks(pair_positions, pair_labels)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
