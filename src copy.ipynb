{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f3cea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "num_agents = 10                   \n",
    "num_rounds = 200\n",
    "num_episodes = 1000                   \n",
    "b = [2, 5, 10]           # Benefit\n",
    "c = 1                    # Cost of cooperation\n",
    "\n",
    "# Learning parameters        \n",
    "# chi = 10 / num_episodes    # Reputation assignment error\n",
    "chi = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03e44c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame(ParallelEnv):\n",
    "    def __init__(self, reward_matrix, agents, norm):\n",
    "        self.agents = agents \n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.norm = norm\n",
    "        self.last_opponent = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.states = {agent: np.random.choice([0,1]) for agent in self.agents}\n",
    "\n",
    "        return self.states\n",
    "    \n",
    "    def get_action_rules(self, action_rule):\n",
    "        # Convert action_rule_id to 4-bits\n",
    "        bits = [(action_rule >> i) & 1 for i in range(4)]  # bits[0]=LSB, bits[3]=MSB\n",
    "        return bits\n",
    "    \n",
    "\n",
    "    def determine_state(self, focal_action, opponent_state):\n",
    "        if focal_action == 0 and opponent_state == 0:\n",
    "            return self.norm[3]  # Bit 3\n",
    "        elif focal_action == 0 and opponent_state == 1:\n",
    "            return self.norm[2]  # Bit 2\n",
    "        elif focal_action == 1 and opponent_state == 0:\n",
    "            return self.norm[1]  # Bit 1\n",
    "        else:  # (1,1)\n",
    "            return self.norm[0]  # Bit 0\n",
    "\n",
    "    def step(self):\n",
    "        pairings = []\n",
    "        players = self.agents.copy()\n",
    "        for _ in range(num_agents//2):\n",
    "            index = random.randrange(len(players))\n",
    "            elem1 = players.pop(index)\n",
    "\n",
    "            index = random.randrange(len(players))\n",
    "            elem2 = players.pop(index)\n",
    "\n",
    "            pairings.append((elem1, elem2))\n",
    "\n",
    "\n",
    "        for pair in pairings:\n",
    "            action1 = pair[0].select_action(self.states[pair[1]])\n",
    "            action2 = pair[1].select_action(self.states[pair[0]])\n",
    "\n",
    "            self.actions[pair[0]] = action1\n",
    "            self.actions[pair[1]] = action2\n",
    "\n",
    "            self.last_opponent[pair[0]] = pair[1]\n",
    "            self.last_opponent[pair[1]] = pair[0]\n",
    "\n",
    "            reward1 = self.reward_matrix[action1][action2]\n",
    "            reward2 = self.reward_matrix[action2][action1]\n",
    "\n",
    "            self.rewards[pair[0]] = reward1\n",
    "            self.rewards[pair[1]] = reward2\n",
    "\n",
    "            state1 = self.determine_state(action1, self.states[pair[1]])\n",
    "            state2 = self.determine_state(action2, self.states[pair[0]])\n",
    "\n",
    "            if (random.random() < chi):\n",
    "                state1 = 1-state1\n",
    "            if (random.random() < chi):\n",
    "                state2 = 1-state2\n",
    "\n",
    "            self.states[pair[0]] = state1\n",
    "            self.states[pair[1]] = state2\n",
    "\n",
    "        return self.states, self.rewards, self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fee9077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner:\n",
    "    \"\"\"A Q-learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seeded=False,\n",
    "        action_size=2,\n",
    "        state_size=2,\n",
    "        learning_rate=0.01,\n",
    "        gamma=0.5,\n",
    "        epsilon=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.seeded = seeded\n",
    "        # initialize the Q-table: (State x Agent Action)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon # exploration\n",
    "\n",
    "        # tracking rewards/progress:\n",
    "        self.rewards_this_episode = []  # during an episode, save every time step's reward\n",
    "        self.episode_total_rewards = []  # each episode, sum the rewards, possibly with a discount factor\n",
    "        self.average_episode_total_rewards = []  # the average (discounted) episode reward to indicate progress\n",
    "\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset_agent(self):\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def select_greedy(self, state):\n",
    "        # np.argmax(self.qtable[state]) will select first entry if two or more Q-values are equal, but we want true randomness:\n",
    "        return np.random.choice(np.flatnonzero(np.isclose(self.qtable[state], self.qtable[state].max())))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.seeded:\n",
    "            return 5 \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action = self.select_greedy(state)\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, new_state, reward, done):\n",
    "        lr = self.learning_rate\n",
    "        self.qtable[state, action] += lr * (reward + (not done) * self.gamma * np.max(self.qtable[new_state]) - self.qtable[state, action])\n",
    "\n",
    "        self.rewards_this_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # track total reward:\n",
    "            episode_reward = self._calculate_episode_reward(self.rewards_this_episode, discount=False)\n",
    "            self.episode_total_rewards.append(episode_reward)\n",
    "\n",
    "            k = len(self.average_episode_total_rewards) + 1  # amount of episodes that have passed\n",
    "            self._calculate_average_episode_reward(k, episode_reward)\n",
    "            \n",
    "            # reset the rewards for the next episode:\n",
    "            self.rewards_this_episode = []\n",
    "\n",
    "    def _calculate_episode_reward(self, rewards_this_episode, discount=False):\n",
    "        if discount:\n",
    "            return sum([self.gamma**i * reward for i, reward in enumerate(rewards_this_episode)])\n",
    "        return sum(rewards_this_episode)\n",
    "\n",
    "    def _calculate_average_episode_reward(self, k, episode_reward):\n",
    "        if k > 1:  # running average is more efficient:\n",
    "            average_episode_reward = (1 - 1 / k) * self.average_episode_total_rewards[-1] + episode_reward / k\n",
    "        else:\n",
    "            average_episode_reward = episode_reward\n",
    "        self.average_episode_total_rewards.append(average_episode_reward)\n",
    "\n",
    "    def print_rewards(self, episode, print_epsilon=True, print_q_table=True):\n",
    "        # print(\"Episode \", episode + 1)\n",
    "        print(\"Total (discounted) reward of this episode: \", self.episode_total_rewards[episode])\n",
    "        print(\"Average total reward over all episodes until now: \", self.average_episode_total_rewards[-1])\n",
    "\n",
    "        print(\"Epsilon:\", self.epsilon) if print_epsilon else None\n",
    "        print(\"Q-table: \", self.qtable) if print_q_table else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef5997f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(payoffs, norm):\n",
    "    run = 46\n",
    "    np.random.seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "    agents = [Qlearner(seeded=True) for _ in range(0)] + [Qlearner(seeded=False) for _ in range(10)]\n",
    "    env = MatrixGame(payoffs, agents, norm)\n",
    "    print(\"Norm:\", env.norm)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%100==0:\n",
    "            print(f\"Number of episodes: {episode}\")\n",
    "        obs = env.reset()\n",
    "        for round in range(num_rounds):\n",
    "            next_obs, rewards, actions = env.step()\n",
    "            last_opponent = env.last_opponent\n",
    "\n",
    "            for agent in agents:\n",
    "                random_opponent = random.choice([a for a in agents if a is not agent])\n",
    "                state = obs[last_opponent[agent]]\n",
    "                new_state = next_obs[random_opponent]\n",
    "                if round == num_rounds-1:\n",
    "                    agent.update(state, actions[agent], new_state, rewards[agent], done=True)\n",
    "                    # agent.update(obs[agent], action_rules[agent], next_obs[agent], rewards[agent], done=True)\n",
    "                else:\n",
    "                    agent.update(state, actions[agent], new_state, rewards[agent], done=False)\n",
    "            obs = next_obs\n",
    "\n",
    "    average_agent_round_payoff = np.zeros(num_episodes//2)\n",
    "    for agent in agents:\n",
    "        average_round_payoff = np.array(agent.episode_total_rewards[num_episodes//2:])/num_rounds\n",
    "        average_agent_round_payoff += average_round_payoff\n",
    "\n",
    "    b = payoffs[0,1]\n",
    "    average_agent_round_payoff = (average_agent_round_payoff.sum()/(10*(num_episodes//2)))/(b-c)\n",
    "\n",
    "    return average_agent_round_payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c47c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Number of episodes: 500\n",
      "Number of episodes: 600\n",
      "Number of episodes: 700\n",
      "Number of episodes: 800\n",
      "Number of episodes: 900\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Number of episodes: 500\n",
      "Number of episodes: 600\n",
      "Number of episodes: 700\n",
      "Number of episodes: 800\n",
      "Number of episodes: 900\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Number of episodes: 500\n",
      "Number of episodes: 600\n",
      "Number of episodes: 700\n",
      "Number of episodes: 800\n",
      "Number of episodes: 900\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Number of episodes: 500\n",
      "Number of episodes: 600\n",
      "Number of episodes: 700\n",
      "Number of episodes: 800\n",
      "Number of episodes: 900\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Number of episodes: 500\n",
      "Number of episodes: 600\n",
      "Number of episodes: 700\n",
      "Number of episodes: 800\n",
      "Number of episodes: 900\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Number of episodes: 300\n",
      "Number of episodes: 400\n",
      "Number of episodes: 500\n",
      "Number of episodes: 600\n",
      "Number of episodes: 700\n",
      "Number of episodes: 800\n",
      "Number of episodes: 900\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "colors = [\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\",]\n",
    "\n",
    "effective_norm =[1,0,0,1]\n",
    "ineffective_norm =[0,0,0,0]\n",
    "\n",
    "payoffs = np.array([[0, b[0]],[-c, b[0]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[1]],[-c, b[1]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[2]],[-c, b[2]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "results = np.array(results)\n",
    "results *= 100\n",
    "\n",
    "x = np.arange(len(results))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(results)), results.flatten(), color=colors)\n",
    "plt.ylim(0,100)\n",
    "pair_labels = [\"2\", \"5\", \"10\"]\n",
    "pair_positions = [(x[i] + x[i+1])/2 for i in range(0, len(x), 2)]\n",
    "plt.xticks(pair_positions, pair_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
