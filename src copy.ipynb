{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "num_agents = 10                   \n",
    "num_rounds = 200\n",
    "num_episodes = 10000                    \n",
    "b = [2, 5, 10]           # Benefit\n",
    "c = 1                    # Cost of cooperation\n",
    "\n",
    "# Learning parameters        \n",
    "# chi = 10 / num_episodes    # Reputation assignment error\n",
    "chi = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e44c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame(ParallelEnv):\n",
    "    def __init__(self, reward_matrix, agents, norm):\n",
    "        self.agents = agents \n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.norm = norm\n",
    "        self.last_opponent = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.states = {agent: np.random.choice([0,1]) for agent in self.agents}\n",
    "\n",
    "        return self.states\n",
    "    \n",
    "    def get_action_rules(self, action_rule):\n",
    "        # Convert action_rule_id to 4-bits\n",
    "        bits = [(action_rule >> i) & 1 for i in range(4)]  # bits[0]=LSB, bits[3]=MSB\n",
    "        return bits\n",
    "    \n",
    "\n",
    "    def determine_state(self, focal_action, opponent_state):\n",
    "        if focal_action == 0 and opponent_state == 0:\n",
    "            return self.norm[3]  # Bit 3\n",
    "        elif focal_action == 0 and opponent_state == 1:\n",
    "            return self.norm[2]  # Bit 2\n",
    "        elif focal_action == 1 and opponent_state == 0:\n",
    "            return self.norm[1]  # Bit 1\n",
    "        else:  # (1,1)\n",
    "            return self.norm[0]  # Bit 0\n",
    "\n",
    "    def step(self):\n",
    "        pairings = []\n",
    "        players = self.agents.copy()\n",
    "        for _ in range(num_agents//2):\n",
    "            index = random.randrange(len(players))\n",
    "            elem1 = players.pop(index)\n",
    "\n",
    "            index = random.randrange(len(players))\n",
    "            elem2 = players.pop(index)\n",
    "\n",
    "            pairings.append((elem1, elem2))\n",
    "\n",
    "\n",
    "        for pair in pairings:\n",
    "            action1 = pair[0].select_action(self.states[pair[1]])\n",
    "            action2 = pair[1].select_action(self.states[pair[0]])\n",
    "\n",
    "            self.actions[pair[0]] = action1\n",
    "            self.actions[pair[1]] = action2\n",
    "\n",
    "            self.last_opponent[pair[0]] = pair[1]\n",
    "            self.last_opponent[pair[1]] = pair[0]\n",
    "\n",
    "            reward1 = self.reward_matrix[action1][action2]\n",
    "            reward2 = self.reward_matrix[action2][action1]\n",
    "\n",
    "            self.rewards[pair[0]] = reward1\n",
    "            self.rewards[pair[1]] = reward2\n",
    "\n",
    "            state1 = self.determine_state(action1, self.states[pair[1]])\n",
    "            state2 = self.determine_state(action2, self.states[pair[0]])\n",
    "\n",
    "            if (random.random() < chi):\n",
    "                state1 = 1-state1\n",
    "            if (random.random() < chi):\n",
    "                state2 = 1-state2\n",
    "\n",
    "            self.states[pair[0]] = state1\n",
    "            self.states[pair[1]] = state2\n",
    "\n",
    "        return self.states, self.rewards, self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fee9077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearner:\n",
    "    \"\"\"A Q-learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seeded=False,\n",
    "        action_size=2,\n",
    "        state_size=2,\n",
    "        learning_rate=0.01,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.seeded = seeded\n",
    "        # initialize the Q-table: (State x Agent Action)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon # exploration\n",
    "\n",
    "        # tracking rewards/progress:\n",
    "        self.rewards_this_episode = []  # during an episode, save every time step's reward\n",
    "        self.episode_total_rewards = []  # each episode, sum the rewards, possibly with a discount factor\n",
    "        self.average_episode_total_rewards = []  # the average (discounted) episode reward to indicate progress\n",
    "\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset_agent(self):\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def select_greedy(self, state):\n",
    "        # np.argmax(self.qtable[state]) will select first entry if two or more Q-values are equal, but we want true randomness:\n",
    "        return np.random.choice(np.flatnonzero(np.isclose(self.qtable[state], self.qtable[state].max())))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.seeded:\n",
    "            return 5 \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action = self.select_greedy(state)\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, new_state, reward, done):\n",
    "        lr = self.learning_rate\n",
    "        self.qtable[state, action] += lr * (reward + (not done) * self.gamma * np.max(self.qtable[new_state]) - self.qtable[state, action])\n",
    "\n",
    "        self.rewards_this_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # track total reward:\n",
    "            episode_reward = self._calculate_episode_reward(self.rewards_this_episode, discount=False)\n",
    "            self.episode_total_rewards.append(episode_reward)\n",
    "\n",
    "            k = len(self.average_episode_total_rewards) + 1  # amount of episodes that have passed\n",
    "            self._calculate_average_episode_reward(k, episode_reward)\n",
    "            \n",
    "            # reset the rewards for the next episode:\n",
    "            self.rewards_this_episode = []\n",
    "\n",
    "    def _calculate_episode_reward(self, rewards_this_episode, discount=False):\n",
    "        if discount:\n",
    "            return sum([self.gamma**i * reward for i, reward in enumerate(rewards_this_episode)])\n",
    "        return sum(rewards_this_episode)\n",
    "\n",
    "    def _calculate_average_episode_reward(self, k, episode_reward):\n",
    "        if k > 1:  # running average is more efficient:\n",
    "            average_episode_reward = (1 - 1 / k) * self.average_episode_total_rewards[-1] + episode_reward / k\n",
    "        else:\n",
    "            average_episode_reward = episode_reward\n",
    "        self.average_episode_total_rewards.append(average_episode_reward)\n",
    "\n",
    "    def print_rewards(self, episode, print_epsilon=True, print_q_table=True):\n",
    "        # print(\"Episode \", episode + 1)\n",
    "        print(\"Total (discounted) reward of this episode: \", self.episode_total_rewards[episode])\n",
    "        print(\"Average total reward over all episodes until now: \", self.average_episode_total_rewards[-1])\n",
    "\n",
    "        print(\"Epsilon:\", self.epsilon) if print_epsilon else None\n",
    "        print(\"Q-table: \", self.qtable) if print_q_table else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5997f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(payoffs, norm):\n",
    "    run = 1\n",
    "    np.random.seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "    agents = [Qlearner(seeded=True) for _ in range(0)] + [Qlearner(seeded=False) for _ in range(10)]\n",
    "    env = MatrixGame(payoffs, agents, norm)\n",
    "    print(\"Norm:\", env.norm)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%100==0:\n",
    "            print(f\"Number of episodes: {episode}\")\n",
    "        obs = env.reset()\n",
    "        for round in range(num_rounds):\n",
    "            next_obs, rewards, actions = env.step()\n",
    "            last_opponent = env.last_opponent\n",
    "\n",
    "            for agent in agents:\n",
    "                state = obs[last_opponent[agent]]\n",
    "                new_state = next_obs[last_opponent[agent]]\n",
    "                if round == num_rounds-1:\n",
    "                    agent.update(state, actions[agent], new_state, rewards[agent], done=True)\n",
    "                    # agent.update(obs[agent], action_rules[agent], next_obs[agent], rewards[agent], done=True)\n",
    "                else:\n",
    "                    agent.update(state, actions[agent], new_state, rewards[agent], done=False)\n",
    "            obs = next_obs\n",
    "\n",
    "    average_agent_round_payoff = np.zeros(num_episodes//2)\n",
    "    for agent in agents:\n",
    "        average_round_payoff = np.array(agent.episode_total_rewards[num_episodes//2:])/num_rounds\n",
    "        average_agent_round_payoff += average_round_payoff\n",
    "\n",
    "    b = payoffs[0,1]\n",
    "    average_agent_round_payoff = (average_agent_round_payoff.sum()/(10*(num_episodes//2)))/(b-c)\n",
    "\n",
    "    return average_agent_round_payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGiCAYAAADNzj2mAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAF5ZJREFUeJzt3X2QVXX9wPHPCgoM8iAkLIxg5NDgs/kQrpIzJuNmRUNSDQ1NaIw2pRYCmfwBZmEUlTkkSjoOOJNW+geazkTDrKZZiE/ZZCliMbFlLJbtrtKwkmxzzm92f6xiPnCX+9m9r9fMmd177tm7371zl/vmex62rrOzszMAABI5qNoDAAB4LYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgD0/UB58MEHY8aMGTF+/Pioq6uLu+66q8f9xZXzly5dGuPGjYshQ4bE9OnTY8uWLT22efHFF2POnDkxfPjwGDlyZMybNy9efvnl/f9pAIDaDJSdO3fGiSeeGKtWrdrn/StWrIiVK1fG6tWrY9OmTTF06NBobGyMXbt2dW9TxMkf/vCH2LBhQ9x7771l9Fx88cX795MAAP1G3f78scBiBmXdunUxc+bM8nbxUMXMysKFC2PRokXlura2thg7dmysXbs2Zs+eHU8//XQcc8wx8eijj8app55abrN+/fr48Ic/HH/961/LrwcAatvASj7Y1q1bY/v27eVunS4jRoyIqVOnxsaNG8tAKT4Wu3W64qRQbH/QQQeVMy4f//jHX/e4HR0d5dJlz5495W6i0aNHl5EEAORXTGS89NJL5WRE8b5/wAKliJNCMWOyt+J2133FxzFjxvQcxMCBMWrUqO5tXmv58uVx9dVXV3KoAECVNDc3xxFHHHHgAqW3LF68OBYsWNB9u9htNHHixPIHLA60BQDya29vjwkTJsSwYcPedNuKBkp9fX35saWlpTyLp0tx+6STTureZseOHT2+7j//+U+5y6br619r0KBB5fJaRZwIFADoW97K4RkVvQ7KpEmTyshoamrqUUvFsSUNDQ3l7eJja2trPP74493b3HfffeVxJcWxKgAAb3sGpbheyXPPPdfjwNgnn3yyPIak2O0yf/78WLZsWUyePLkMliVLlpQHw3Sd6XP00UfHhz70objooovKU5F3794dl156aXkArTN4AIB3FCiPPfZYnH322d23u44NmTt3bnkq8RVXXFFeK6W4rkkxUzJt2rTyNOLBgwd3f81tt91WRsk555xTHsU7a9as8topAAD7fR2Uail2GxWnLxcHyzoGBQD63/u3v8UDAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA/T9QXn311ViyZElMmjQphgwZEkcddVR84xvfiM7Ozu5tis+XLl0a48aNK7eZPn16bNmypdJDAQD6qIoHyre//e248cYb4/rrr4+nn366vL1ixYr4wQ9+0L1NcXvlypWxevXq2LRpUwwdOjQaGxtj165dlR4OANAH1XXuPbVRAR/96Edj7Nixccstt3SvmzVrVjlT8qMf/aicPRk/fnwsXLgwFi1aVN7f1tZWfs3atWtj9uzZb/o92tvbY8SIEeXXDR8+vJLDBwB6ydt5/674DMoZZ5wRTU1N8eyzz5a3f/e738VDDz0U5513Xnl769atsX379nK3TpdisFOnTo2NGzfu8zE7OjrKH2rvBQDovwZW+gGvvPLKMiCmTJkSAwYMKI9Jueaaa2LOnDnl/UWcFIoZk70Vt7vue63ly5fH1VdfXemhAgBJVXwG5Y477ojbbrstbr/99njiiSfi1ltvje9+97vlx3dq8eLF5XRQ19Lc3FzRMQMA/XwG5Stf+Uo5i9J1LMnxxx8ff/nLX8pZkLlz50Z9fX25vqWlpTyLp0tx+6STTtrnYw4aNKhcAIDaUPEZlH//+99x0EE9H7bY1bNnz57y8+L04yJSiuNUuhS7hIqzeRoaGio9HACgD6r4DMqMGTPKY04mTpwYxx57bPz2t7+Na6+9Nj73uc+V99fV1cX8+fNj2bJlMXny5DJYiuumFGf2zJw5s9LDAQD6oIoHSnG9kyI4vvjFL8aOHTvK8Pj85z9fXpityxVXXBE7d+6Miy++OFpbW2PatGmxfv36GDx4cKWHAwD0QRW/DsqB4DooAND3VPU6KAAA+0ugAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQBqI1D+9re/xWc+85kYPXp0DBkyJI4//vh47LHHuu/v7OyMpUuXxrhx48r7p0+fHlu2bOmNoQAAfVDFA+Vf//pXnHnmmXHwwQfHz3/+8/jjH/8Y3/ve9+Kwww7r3mbFihWxcuXKWL16dWzatCmGDh0ajY2NsWvXrkoPBwDog+o6i+mMCrryyivj17/+dfzqV7/a5/3Ftxs/fnwsXLgwFi1aVK5ra2uLsWPHxtq1a2P27Nlv+j3a29tjxIgR5dcNHz68ksMHAHrJ23n/rvgMys9+9rM49dRT45Of/GSMGTMm3ve+98XNN9/cff/WrVtj+/bt5W6dLsVgp06dGhs3btznY3Z0dJQ/1N4LANB/VTxQ/vznP8eNN94YkydPjl/84hfxhS98Ib70pS/FrbfeWt5fxEmhmDHZW3G7677XWr58eRkxXcuECRMqPWwAoD8Hyp49e+Lkk0+Ob37zm+XsycUXXxwXXXRRebzJO7V48eJyOqhraW5uruiYAYB+HijFmTnHHHNMj3VHH310bNu2rfy8vr6+/NjS0tJjm+J2132vNWjQoHJf1d4LANB/VTxQijN4Nm/e3GPds88+G0ceeWT5+aRJk8oQaWpq6r6/OKakOJunoaGh0sMBAPqggZV+wMsvvzzOOOOMchfPpz71qXjkkUfipptuKpdCXV1dzJ8/P5YtW1Yep1IEy5IlS8oze2bOnFnp4QAAfVDFA+W0006LdevWlceNfP3rXy8D5Lrrros5c+Z0b3PFFVfEzp07y+NTWltbY9q0abF+/foYPHhwpYcDAPRBFb8OyoHgOigA0PdU9TooAAD7S6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABAGovUL71rW9FXV1dzJ8/v3vdrl274pJLLonRo0fHoYceGrNmzYqWlpbeHgoA0Ef0aqA8+uij8cMf/jBOOOGEHusvv/zyuOeee+LOO++MBx54IJ5//vk4//zze3MoAEAf0muB8vLLL8ecOXPi5ptvjsMOO6x7fVtbW9xyyy1x7bXXxgc/+ME45ZRTYs2aNfGb3/wmHn744X0+VkdHR7S3t/dYAID+q9cCpdiF85GPfCSmT5/eY/3jjz8eu3fv7rF+ypQpMXHixNi4ceM+H2v58uUxYsSI7mXChAm9NWwAoL8Gyk9+8pN44oknyrB4re3bt8chhxwSI0eO7LF+7Nix5X37snjx4nLmpWtpbm7ujWEDAEkMrPQDFvHw5S9/OTZs2BCDBw+uyGMOGjSoXACA2lDxGZRiF86OHTvi5JNPjoEDB5ZLcSDsypUry8+LmZJXXnklWltbe3xdcRZPfX19pYcDAPRBFZ9BOeecc+L3v/99j3UXXnhheZzJV7/61fL4kYMPPjiamprK04sLmzdvjm3btkVDQ0OlhwMA9EEVD5Rhw4bFcccd12Pd0KFDy2uedK2fN29eLFiwIEaNGhXDhw+Pyy67rIyT008/vdLDAQD6oIoHylvx/e9/Pw466KByBqU4hbixsTFuuOGGagwFAEiorrOzszP6mOI6KMXpxsUZPcUMDADQv96//S0eACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hlY7QEAQFp1dVGzOjur+u3NoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoDqz0AqGV1dVGTOjurPYLa5PVGX2IGBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAOj/gbJ8+fI47bTTYtiwYTFmzJiYOXNmbN68ucc2u3btiksuuSRGjx4dhx56aMyaNStaWloqPRQAoI+qeKA88MADZXw8/PDDsWHDhti9e3ece+65sXPnzu5tLr/88rjnnnvizjvvLLd//vnn4/zzz6/0UACAPqqus7Ozsze/wQsvvFDOpBQhctZZZ0VbW1scfvjhcfvtt8cnPvGJcptnnnkmjj766Ni4cWOcfvrpr3uMjo6OcunS3t4eEyZMKB9r+PDhvTl86FV1dVGTevdfHd6I19s7UKtPWi/9ohbv3yNGjHhL79+9fgxKMYjCqFGjyo+PP/54Oasyffr07m2mTJkSEydOLAPljXYbFT9Q11LECQDQf/VqoOzZsyfmz58fZ555Zhx33HHluu3bt8chhxwSI0eO7LHt2LFjy/v2ZfHixWXodC3Nzc29OWwAoMoG9uaDF8eiPPXUU/HQQw/t1+MMGjSoXACA2tBrMyiXXnpp3HvvvXH//ffHEUcc0b2+vr4+XnnllWhtbe2xfXEWT3EfAEDFA6U45raIk3Xr1sV9990XkyZN6nH/KaecEgcffHA0NTV1rytOQ962bVs0NDRUejgAQB80sDd26xRn6Nx9993ltVC6jispDm4dMmRI+XHevHmxYMGC8sDZ4ijeyy67rIyTfZ3BAwDUnoqfZlz3BqdkrVmzJi644ILuC7UtXLgwfvzjH5enDzc2NsYNN9zwlnfxvJ3TlCCzWj2D0WnG1eH19g7U6pOW4DTjXr8OSm8QKPQXtfpvX9/7V6d/8Hp7B2r1SauF66AAALxdAgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hEoAEA6AgUASEegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIR6AAAOkIFAAgHYECAKQjUACAdAQKAJCOQAEA0hlY7QFkVFcXNamzcz++uFaftP1+4nhHavX15rVGDTGDAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIApCNQAIB0BAoAkI5AAQDSESgAQDoCBQBIp6qBsmrVqnj3u98dgwcPjqlTp8YjjzxSzeEAALUeKD/96U9jwYIFcdVVV8UTTzwRJ554YjQ2NsaOHTuqNSQAIIm6zs7Ozmp842LG5LTTTovrr7++vL1nz56YMGFCXHbZZXHllVf22Lajo6NcurS1tcXEiROjubk5hg8fXvGxjRgRNamtbT++uFaftP184mr1aduv11rBE/eOeNregVp90iryi/p67e3t5Xt9a2trjHiz57azCjo6OjoHDBjQuW7duh7rP/vZz3Z+7GMfe932V111VRFRFovFYrFYou8vzc3Nb9oKA6MK/vGPf8Srr74aY8eO7bG+uP3MM8+8bvvFixeXu4O6FLMtL774YowePTrq6uoOyJhrXVf19tasFXTxWuNA8no7sIqdNi+99FKMHz/+TbetSqC8XYMGDSqXvY0cObJq46llxS+wX2IOBK81DiSvtwPnTXftVPMg2Xe9610xYMCAaGlp6bG+uF1fX1+NIQEAiVQlUA455JA45ZRToqmpqcdum+J2Q0NDNYYEACRStV08xTElc+fOjVNPPTXe//73x3XXXRc7d+6MCy+8sFpD4n8odrEVp4S/dlcbVJrXGgeS11teVTvNuFCcYvyd73wntm/fHieddFKsXLmyPP0YAKhtVQ0UAIB98bd4AIB0BAoAkI5AAQDSESgAQDoChTe0fPny8g86Dhs2LMaMGRMzZ86MzZs3V3tY9FNf+9rXyj9dsfcyZcqUag+LfuDBBx+MGTNmlJdXL15Xd911V4/7i3NFli5dGuPGjYshQ4bE9OnTY8uWLVUbL/9HoPCGHnjggbjkkkvi4Ycfjg0bNsTu3bvj3HPPLa9XA73h2GOPjb///e/dy0MPPVTtIdEPFP9mnXjiibFq1ap93r9ixYryMherV6+OTZs2xdChQ6OxsTF27dp1wMfK/3OaMW/ZCy+8UM6kFOFy1llnVXs49MMZlOJ/tk8++WS1h0I/VsygrFu3rpwRLhRvgcXMysKFC2PRokXlura2tvKP165duzZmz55d5RHXLjMovGXFL21h1KhR1R4K/VQxrV68WbznPe+JOXPmxLZt26o9JPq5rVu3lhcLLXbr7P3H7IqLhm7cuLGqY6t1AoW3pPhbSfPnz48zzzwzjjvuuGoPh36oeEMo/se6fv36uPHGG8s3jg984APln2aH3lLESaGYMdlbcbvrPmrsb/HQtxTHojz11FOOCaDXnHfeed2fn3DCCWWwHHnkkXHHHXfEvHnzqjo24MAzg8KbuvTSS+Pee++N+++/P4444ohqD4caMXLkyHjve98bzz33XLWHQj9WX19ffmxpaemxvrjddR/VIVB4Q8XBY0WcFAeU3XfffTFp0qRqD4ka8vLLL8ef/vSn8tRP6C3Fv2tFiDQ1NXWva29vL8/maWhoqOrYap1dPPzP3Tq333573H333eW1ULr2xxYHkBXXCoBKKs6gKK5VUezWef755+Oqq66KAQMGxKc//elqD41+ELt7z8QVxzcVZ4sVB/xPnDixPL5u2bJlMXny5DJYlixZUh6s3XWmD9XhNGP+5+l4+7JmzZq44IILDvh46N+K0zmLC2r985//jMMPPzymTZsW11xzTRx11FHVHhp93C9/+cs4++yzX7d+7ty55YHZxdtgEcQ33XRTtLa2lq+9G264odzFSPUIFAAgHcegAADpCBQAIB2BAgCkI1AAgHQECgCQjkABANIRKABAOgIFAEhHoAAA6QgUACAdgQIARDb/BTmPw+u7QBo1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "colors = [\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\",]\n",
    "\n",
    "effective_norm =[1,0,0,1]\n",
    "ineffective_norm =[0,0,0,0]\n",
    "\n",
    "payoffs = np.array([[0, b[0]],[-c, b[0]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[1]],[-c, b[1]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[2]],[-c, b[2]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "results = np.array(results)\n",
    "results *= 100\n",
    "\n",
    "x = np.arange(len(results))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(results)), results.flatten(), color=colors)\n",
    "plt.ylim(0,100)\n",
    "pair_labels = [\"2\", \"5\", \"10\"]\n",
    "pair_positions = [(x[i] + x[i+1])/2 for i in range(0, len(x), 2)]\n",
    "plt.xticks(pair_positions, pair_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
