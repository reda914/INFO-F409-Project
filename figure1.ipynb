{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f3cea02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.330905Z",
     "start_time": "2025-12-27T13:49:13.325057Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "num_agents = 10                   \n",
    "num_rounds = 200\n",
    "num_episodes = 250\n",
    "b = [2, 5, 10]           # Benefit\n",
    "c = 1                    # Cost of cooperation\n",
    "\n",
    "# Learning parameters        \n",
    "# chi = 10 / num_episodes    # Reputation assignment error\n",
    "chi = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03e44c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.370167Z",
     "start_time": "2025-12-27T13:49:13.356360Z"
    }
   },
   "outputs": [],
   "source": [
    "class MatrixGame(ParallelEnv):\n",
    "    def __init__(self, reward_matrix, agents, norm):\n",
    "        self.agents = agents \n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.norm = norm\n",
    "        self.last_opponent = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.states = {agent: np.random.choice([0,1]) for agent in self.agents}\n",
    "\n",
    "        return self.states\n",
    "    \n",
    "    def get_action_rules(self, action_rule):\n",
    "        # Convert action_rule_id to 4-bits\n",
    "        bits = [(action_rule >> i) & 1 for i in range(4)]  # bits[0]=LSB, bits[3]=MSB\n",
    "        return bits\n",
    "    \n",
    "\n",
    "    def determine_state(self, focal_action, opponent_state):\n",
    "        if focal_action == 0 and opponent_state == 0:\n",
    "            return self.norm[3]  # Bit 3\n",
    "        elif focal_action == 0 and opponent_state == 1:\n",
    "            return self.norm[2]  # Bit 2\n",
    "        elif focal_action == 1 and opponent_state == 0:\n",
    "            return self.norm[1]  # Bit 1\n",
    "        else:  # (1,1)\n",
    "            return self.norm[0]  # Bit 0\n",
    "\n",
    "    def step(self):\n",
    "        pairings = []\n",
    "        players = self.agents.copy()\n",
    "        for _ in range(num_agents//2):\n",
    "            index = random.randrange(len(players))\n",
    "            elem1 = players.pop(index)\n",
    "\n",
    "            index = random.randrange(len(players))\n",
    "            elem2 = players.pop(index)\n",
    "\n",
    "            pairings.append((elem1, elem2))\n",
    "\n",
    "\n",
    "        for pair in pairings:\n",
    "            action1 = pair[0].select_action(self.states[pair[1]])\n",
    "            action2 = pair[1].select_action(self.states[pair[0]])\n",
    "\n",
    "            self.actions[pair[0]] = action1\n",
    "            self.actions[pair[1]] = action2\n",
    "\n",
    "            self.last_opponent[pair[0]] = pair[1]\n",
    "            self.last_opponent[pair[1]] = pair[0]\n",
    "\n",
    "            reward1 = self.reward_matrix[action1][action2]\n",
    "            reward2 = self.reward_matrix[action2][action1]\n",
    "\n",
    "            self.rewards[pair[0]] = reward1\n",
    "            self.rewards[pair[1]] = reward2\n",
    "\n",
    "            state1 = self.determine_state(action1, self.states[pair[1]])\n",
    "            state2 = self.determine_state(action2, self.states[pair[0]])\n",
    "\n",
    "            if (random.random() < chi):\n",
    "                state1 = 1-state1\n",
    "            if (random.random() < chi):\n",
    "                state2 = 1-state2\n",
    "\n",
    "            self.states[pair[0]] = state1\n",
    "            self.states[pair[1]] = state2\n",
    "\n",
    "        return self.states, self.rewards, self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fee9077c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.392723Z",
     "start_time": "2025-12-27T13:49:13.379535Z"
    }
   },
   "outputs": [],
   "source": [
    "class Qlearner:\n",
    "    \"\"\"A Q-learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seeded=False,\n",
    "        action_size=2,\n",
    "        state_size=2,\n",
    "        learning_rate=0.01,\n",
    "        gamma=0.8,\n",
    "        epsilon=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.seeded = seeded\n",
    "        # initialize the Q-table: (State x Agent Action)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon # exploration\n",
    "\n",
    "        # tracking rewards/progress:\n",
    "        self.rewards_this_episode = []  # during an episode, save every time step's reward\n",
    "        self.episode_total_rewards = []  # each episode, sum the rewards, possibly with a discount factor\n",
    "        self.average_episode_total_rewards = []  # the average (discounted) episode reward to indicate progress\n",
    "\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset_agent(self):\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def select_greedy(self, state):\n",
    "        # np.argmax(self.qtable[state]) will select first entry if two or more Q-values are equal, but we want true randomness:\n",
    "        return np.random.choice(np.flatnonzero(np.isclose(self.qtable[state], self.qtable[state].max())))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.seeded:\n",
    "            return 5 \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action = self.select_greedy(state)\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, new_state, reward, done):\n",
    "        lr = self.learning_rate\n",
    "        self.qtable[state, action] += lr * (reward + (not done) * self.gamma * np.max(self.qtable[new_state]) - self.qtable[state, action])\n",
    "\n",
    "        self.rewards_this_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # track total reward:\n",
    "            episode_reward = self._calculate_episode_reward(self.rewards_this_episode, discount=False)\n",
    "            self.episode_total_rewards.append(episode_reward)\n",
    "\n",
    "            k = len(self.average_episode_total_rewards) + 1  # amount of episodes that have passed\n",
    "            self._calculate_average_episode_reward(k, episode_reward)\n",
    "            \n",
    "            # reset the rewards for the next episode:\n",
    "            self.rewards_this_episode = []\n",
    "\n",
    "    def _calculate_episode_reward(self, rewards_this_episode, discount=False):\n",
    "        if discount:\n",
    "            return sum([self.gamma**i * reward for i, reward in enumerate(rewards_this_episode)])\n",
    "        return sum(rewards_this_episode)\n",
    "\n",
    "    def _calculate_average_episode_reward(self, k, episode_reward):\n",
    "        if k > 1:  # running average is more efficient:\n",
    "            average_episode_reward = (1 - 1 / k) * self.average_episode_total_rewards[-1] + episode_reward / k\n",
    "        else:\n",
    "            average_episode_reward = episode_reward\n",
    "        self.average_episode_total_rewards.append(average_episode_reward)\n",
    "\n",
    "    def print_rewards(self, episode, print_epsilon=True, print_q_table=True):\n",
    "        # print(\"Episode \", episode + 1)\n",
    "        print(\"Total (discounted) reward of this episode: \", self.episode_total_rewards[episode])\n",
    "        print(\"Average total reward over all episodes until now: \", self.average_episode_total_rewards[-1])\n",
    "\n",
    "        print(\"Epsilon:\", self.epsilon) if print_epsilon else None\n",
    "        print(\"Q-table: \", self.qtable) if print_q_table else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5997f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:49:13.427481Z",
     "start_time": "2025-12-27T13:49:13.419026Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(payoffs, norm):\n",
    "    run = 46\n",
    "    np.random.seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "    agents = [Qlearner(seeded=True) for _ in range(0)] + [Qlearner(seeded=False) for _ in range(10)]\n",
    "    env = MatrixGame(payoffs, agents, norm)\n",
    "    print(\"Norm:\", env.norm)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if episode%100==0:\n",
    "            print(f\"Number of episodes: {episode}\")\n",
    "        obs = env.reset()\n",
    "        prev_obs = None\n",
    "        prev_actions = {}\n",
    "        prev_rewards = {}\n",
    "        prev_opponents = {}\n",
    "        for round in range(num_rounds):\n",
    "            \n",
    "            next_obs, rewards, actions = env.step()\n",
    "            last_opponent = env.last_opponent\n",
    "            if prev_obs is not None:\n",
    "                for agent in agents:\n",
    "                    s = prev_obs[prev_opponents[agent]]\n",
    "                    a = prev_actions[agent]\n",
    "                    r = prev_rewards[agent]\n",
    "                    s_next = obs[last_opponent[agent]]\n",
    "                    agent.update(s, a, s_next, r, done=False)\n",
    "            prev_obs = obs\n",
    "            prev_actions = actions\n",
    "            prev_rewards = rewards\n",
    "            prev_opponents = last_opponent\n",
    "            obs = next_obs\n",
    "\n",
    "        for agent in agents:\n",
    "            s = prev_obs[prev_opponents[agent]]\n",
    "            a = prev_actions[agent]\n",
    "            r = prev_rewards[agent]\n",
    "            s_next = obs[last_opponent[agent]]\n",
    "\n",
    "            agent.update(s, a, s_next, r, done=True)\n",
    "\n",
    "    average_agent_round_payoff = np.zeros(num_episodes//2)\n",
    "    for agent in agents:\n",
    "        average_round_payoff = np.array(agent.episode_total_rewards[num_episodes//2:])/num_rounds\n",
    "        average_agent_round_payoff += average_round_payoff\n",
    "\n",
    "    b = payoffs[0,1]\n",
    "    average_agent_round_payoff = (average_agent_round_payoff.sum()/(10*(num_episodes//2)))/(b-c)\n",
    "\n",
    "    return average_agent_round_payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c47c01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T13:52:21.179913Z",
     "start_time": "2025-12-27T13:49:13.427481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Norm: [1, 0, 0, 1]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n",
      "Norm: [0, 0, 0, 0]\n",
      "Number of episodes: 0\n",
      "Number of episodes: 100\n",
      "Number of episodes: 200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "colors = [\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\",]\n",
    "\n",
    "effective_norm =[1,0,0,1]\n",
    "ineffective_norm =[0,0,0,0]\n",
    "\n",
    "payoffs = np.array([[0, b[0]],[-c, b[0]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[1]],[-c, b[1]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "\n",
    "payoffs = np.array([[0, b[2]],[-c, b[2]-c]])\n",
    "results.append(run(payoffs, effective_norm))\n",
    "results.append(run(payoffs, ineffective_norm))\n",
    "\n",
    "results = np.array(results)\n",
    "results *= 100\n",
    "\n",
    "x = np.arange(len(results))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(len(results)), results.flatten(), color=colors)\n",
    "plt.ylim(0,100)\n",
    "pair_labels = [\"2\", \"5\", \"10\"]\n",
    "pair_positions = [(x[i] + x[i+1])/2 for i in range(0, len(x), 2)]\n",
    "plt.xticks(pair_positions, pair_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
